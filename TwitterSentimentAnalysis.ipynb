{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b316a0",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b2763c",
   "metadata": {},
   "source": [
    "### Step 1: Installing and Setting up Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b33600e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\lahar\\anaconda3\\lib\\site-packages (1.6.14)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from kaggle) (2023.7.22)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from kaggle) (4.65.0)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from kaggle) (1.26.16)\n",
      "Requirement already satisfied: bleach in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from kaggle) (4.1.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from bleach->kaggle) (23.1)\n",
      "Requirement already satisfied: webencodings in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from requests->kaggle) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from requests->kaggle) (3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lahar\\anaconda3\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install the kaggle library\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f7b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a kaggle directory and copy the kaggle.json file to it\n",
    "import os\n",
    "if not os.path.exists(os.path.expanduser(\"~/.kaggle\")):\n",
    "    os.makedirs(os.path.expanduser(\"~/.kaggle\"))\n",
    "import shutil\n",
    "shutil.copy('kaggle.json', os.path.expanduser(\"~/.kaggle/\"))\n",
    "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2274e8",
   "metadata": {},
   "source": [
    "### Step 2: Importing Twitter Sentiment Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59c492cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
      "License(s): other\n",
      "sentiment140.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
     ]
    }
   ],
   "source": [
    "# Download the dataset from Kaggle\n",
    "!kaggle datasets download -d kazanova/sentiment140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07201ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset is extracted\n"
     ]
    }
   ],
   "source": [
    "# Extract the compressed dataset\n",
    "from zipfile import ZipFile\n",
    "dataset = \"sentiment140.zip\"\n",
    "\n",
    "with ZipFile(dataset, 'r') as zip:\n",
    "    zip.extractall()\n",
    "    print(\"The dataset is extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f2b46",
   "metadata": {},
   "source": [
    "### Step 3: Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d68695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59e6da05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lahar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download the stopwords from nltk\n",
    "nltk.download('stopwords')\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8439db12",
   "metadata": {},
   "source": [
    "### Step 4: Loading and Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a42c3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the CSV file to a pandas dataframe\n",
    "twitter_data = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7673a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599999, 6)\n"
     ]
    }
   ],
   "source": [
    "# Check the number of rows and columns\n",
    "print(twitter_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a28c352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
      "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
      "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
      "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
      "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
      "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
      "\n",
      "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
      "0  is upset that he can't update his Facebook by ...                                                                   \n",
      "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
      "2    my whole body feels itchy and like its on fire                                                                    \n",
      "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
      "4                      @Kwesidei not the whole crew                                                                    \n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 rows of the dataframe\n",
    "print(twitter_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d951aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns and read the dataset again\n",
    "column_names = ['target', 'id', 'date', 'flag', 'user', 'text']\n",
    "twitter_data = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='ISO-8859-1', names=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d96a3ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 6)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of the dataframe\n",
    "print(twitter_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fc2a9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target    0\n",
      "id        0\n",
      "date      0\n",
      "flag      0\n",
      "user      0\n",
      "text      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count the number of missing values in the dataset\n",
    "print(twitter_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0ca3116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    800000\n",
      "4    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of the target column\n",
    "print(twitter_data['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fea50bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target\n",
      "0    800000\n",
      "1    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert the target label \"4\" to \"1\"\n",
    "twitter_data.replace({'target': {4: 1}}, inplace=True)\n",
    "print(twitter_data['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7ea4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 ---> Negative Tweet\n",
    "# 1 ---> Positive Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c406c83e",
   "metadata": {},
   "source": [
    "### Step 5: Data Preprocessing and Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "706438ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Porter Stemmer\n",
    "port_stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfc50aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stemming function\n",
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower()\n",
    "    stemmed_content = stemmed_content.split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4df7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming to the 'text' column\n",
    "twitter_data['stemmed_content'] = twitter_data['text'].apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b6ffee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the dataframe\n",
    "print(twitter_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39f1dc6",
   "metadata": {},
   "source": [
    "### Step 6: Separating Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77029d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = twitter_data['stemmed_content'].values\n",
    "Y = twitter_data['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa49977",
   "metadata": {},
   "source": [
    "### Step 7: Splitting the Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde6f8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90110342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shapes of the datasets\n",
    "print(X.shape, X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0877f4",
   "metadata": {},
   "source": [
    "### Step 8: Converting Textual Data to Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83d3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab416303",
   "metadata": {},
   "source": [
    "### Step 9: Training the Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6408b0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Logistic Regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006a9dd8",
   "metadata": {},
   "source": [
    "### Step 10: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy score on the training data\n",
    "X_train_prediction = model.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(Y_train, X_train_prediction)\n",
    "print('Accuracy score on the training data : ', training_data_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9356bb30",
   "metadata": {},
   "source": [
    "### Step 11: Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f4679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'trained_model.sav'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6dc224",
   "metadata": {},
   "source": [
    "### Step 12: Using the Model for Future Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab3eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "loaded_model = pickle.load(open('trained_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1e49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the loaded model on the test data\n",
    "result = loaded_model.score(X_test, Y_test)\n",
    "print('Model accuracy on test data: ', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d3b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new data\n",
    "def make_prediction(index):\n",
    "    X_new = X_test[index]\n",
    "    print('Actual label:', Y_test[index])\n",
    "    prediction = loaded_model.predict(X_new)\n",
    "    print('Prediction:', prediction)\n",
    "    if prediction[0] == 0:\n",
    "        print('Negative tweet')\n",
    "    else:\n",
    "        print('Positive tweet')\n",
    "        \n",
    "make_prediction(200)\n",
    "make_prediction(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c464790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308abf9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
